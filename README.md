Awesome. I’ll dig into the Hedgehog project starting with their public GitHub repositories and documentation, and compile a comprehensive guide on how to build and deploy their SONiC-based network fabric on Dell Broadcom switches.

This will include building the switch OS images, deploying the Fabricator, Gateway, and Dataplane components, and covering both bare metal and container-based workflows for production and development use.

I’ll let you know when the guide is ready for review.

# Hedgehog SONiC-Based Network Deployment Guide (Dell Broadcom Switches)

## Overview of Hedgehog Open Network Fabric

Hedgehog’s Open Network Fabric is an open-source networking platform built on the **SONiC** network OS (Software for Open Networking in the Cloud). It brings cloud-like virtual networking (VPCs, multi-tenant isolation, etc.) to on-premises environments using commodity switch hardware ([Hedgehog · GitHub](https://github.com/githedgehog#:~:text=The%20Hedgehog%20Open%20Network%20Fabric,in)). Under the hood, Hedgehog uses **standard BGP EVPN and VXLAN** for the fabric overlay network, enabling creation of virtual networks (VPCs) on top of a physical leaf-spine underlay ([Fabric Implementation - Open Network Fabric](https://docs.githedgehog.com/latest/architecture/fabric/#:~:text=The%20Hedgehog%20Open%20Network%20Fabric,abstractions%20starting%20from%20the%20physical)) ([Fabric Implementation - Open Network Fabric](https://docs.githedgehog.com/latest/architecture/fabric/#:~:text=in%20the%20Hedgehog%20Fabric%20by,virtual%20networks%20over%20the%20same)). All user-facing management is done via Kubernetes Custom Resources (CRDs) – the fabric is managed through a Kubernetes control plane, providing a familiar cloud-native interface to configure network intent (VPCs, subnets, attachments, etc.) ([Hedgehog · GitHub](https://github.com/githedgehog#:~:text=The%20Open%20Network%20Fabric%20is,manage%20resources%20in%20the%20fabric)). This guide will cover how to build the custom SONiC-based switch OS images and deploy the Hedgehog fabric components – **Fabricator**, **Dataplane**, and **Gateway** – on Dell switches with Broadcom ASICs.

**Supported Dell Switch Models:** Hedgehog supports several Dell **ON-Series** switches that use Broadcom ASICs. These include Dell *S5232F-ON* (32×100GbE spine/leaf), *S5248F-ON* (48×25GbE + uplinks leaf), and *Z9332F-ON* (32×400GbE spine) among others ([Supported Devices - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/supported-devices/#:~:text=,C4632SB)) ([Supported Devices - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/supported-devices/#:~:text=,C4632SB)). Ensure you are using a supported model and have the **ONIE** bootloader available on the switch (these models come with ONIE for third-party NOS installation).

**Primary Hedgehog Components:**

- **Fabricator (hhfab CLI):** The Hedgehog Fabricator is a CLI tool (`hhfab`) used to build installation images and orchestrate the deployment. You will use `hhfab` to generate configuration, build the custom SONiC installer image, and manage virtual lab setups ([Download - Open Network Fabric](https://docs.githedgehog.com/latest/getting-started/download/#:~:text=The%20main%20entry%20point%20for,or%20run%20the%20Virtual%20LAB)).
- **Fabric Control Node:** A server (or VM) that acts as the control plane for the fabric. This node runs Kubernetes and Hedgehog’s controllers (the Fabric controller and Gateway controller) which translate CRDs into network configs. In production, use a dedicated control node with server-class specs (e.g. 8-core CPU, 32GB RAM, NVMe SSD) ([System Requirements - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/requirements/#:~:text=,Micron%207450%20MAX%20400GB%20NVMe)). *Note:* For high availability, future versions may support multiple control nodes, but a single node is the current minimal setup.
- **Dataplane (Fabric Agents on Switches):** The physical switches run Hedgehog’s custom SONiC-based OS. Each switch runs a Hedgehog **Fabric Agent** that interfaces with the control plane to apply configurations (VLANs, VRFs, BGP sessions, etc.) to the device ([System Requirements - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/requirements/#:~:text=,Agent%20that%20controls%20devices%20configuration)). The agent ensures the switch’s data plane (Broadcom ASIC) is programmed according to the desired state (for example, establishing EVPN sessions or configuring QoS for RoCE).
- **Gateway:** The Hedgehog *gateway* functionality provides connectivity between the fabric and external networks. The Gateway component (API, controller, and agent) handles “transit” networking – e.g. peering with external routers, north-south traffic, load balancers, etc. ([Hedgehog AI Network](https://hedgehog.cloud#:~:text=Hedgehog%20offers%20a%20cloud%20user,transit%2C%20load%20balancing%20and%20security)). In practice, a **border leaf** switch in the fabric can serve as a gateway by connecting to an outside network device. Hedgehog’s Gateway controller and agent will configure the necessary BGP/routing on the designated gateway switch port (or on a dedicated gateway appliance if used) to exchange routes between your fabric’s VPCs and the external network.

Below, we detail the steps to **build the SONiC images** for Dell/Broadcom switches using Hedgehog’s tooling and then **deploy the fabric** step-by-step in both a bare-metal environment and a containerized virtual lab. We also include prerequisites, example configurations, and best practices for production and learning setups.

## Prerequisites and Preparation

Before beginning, ensure you have the following:

- **Build Server Requirements:** A Linux host (Ubuntu 22.04 recommended) with internet access to serve as the build machine for `hhfab`. Minimum 8 GB RAM and 25 GB free disk space are required for building the installer ([Install - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/install/#:~:text=,it%20to%20install%20the%20OS)). Install Docker (17.03+), Git, and Go (v1.24+). Hedgehog also uses a tool called **Zot** (OCI registry cache) and **just** (build script runner) on the build host ([GitHub - githedgehog/fabricator: Hedgehog Open Network Fabric Installer](https://github.com/githedgehog/fabricator#:~:text=Prerequisites)). Ensure you have a GitHub account and obtain access credentials for Hedgehog’s container registry (GHCR) if required (prior to GA, Hedgehog artifacts require a partner token) ([Download - Open Network Fabric](https://docs.githedgehog.com/latest/getting-started/download/#:~:text=Getting%20access)). Log in to the registry with `docker login ghcr.io` using the provided credentials ([Download - Open Network Fabric](https://docs.githedgehog.com/latest/getting-started/download/#:~:text=Hedgehog%20Support%20Portal)) before pulling any images.
- **Hedgehog Fabricator CLI:** Download or build the `hhfab` CLI tool on the build server. (If you have access, you can pull a release of hhfab from Hedgehog’s GHCR; alternatively, build from the open-source `fabricator` repo). Once installed, confirm you can run `hhfab --help` ([Download - Open Network Fabric](https://docs.githedgehog.com/latest/getting-started/download/#:~:text=The%20main%20entry%20point%20for,or%20run%20the%20Virtual%20LAB)).
- **Control Node Hardware:** Prepare a machine to be the **Fabric Control Node** (bare metal server preferred). It will run a lightweight Linux (Flatcar Container Linux) and Kubernetes. Hedgehog’s reference control node is an 8-core AMD EPYC with 32 GB RAM and NVMe SSD ([System Requirements - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/requirements/#:~:text=,Micron%207450%20MAX%20400GB%20NVMe)), but any comparable x86_64 server should suffice. Ensure IPMI or console access for OS installation. Plan to connect this control node to the out-of-band management network (see next bullet).
- **Out-of-Band Management Network:** Set up an isolated management network for the control node and switches ([System Requirements - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/requirements/#:~:text=In%20order%20to%20provision%20and,connect%20to%20the%20control%20node)). Typically this is a dedicated gigabit Ethernet switch that all device management ports connect to. No other devices/users should be on this network for security. It’s recommended to use at least a 10GbE uplink from this management switch to the control node, to handle image transfers quickly ([System Requirements - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/requirements/#:~:text=In%20order%20to%20provision%20and,connect%20to%20the%20control%20node)). Assign an IP address to the control node’s management interface (either via DHCP or static) – this network will be used by Hedgehog to provision and communicate with the switches.
- **Dell Switches with ONIE:** Rack and cable your Dell switches. Connect each switch’s management port to the management network switch. **Record the serial numbers and management MAC addresses** of each switch – you will need these for the Hedgehog wiring diagram so that the software can identify each device ([Install - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/install/#fabric-manages-switches#:~:text=4,to%20have%20them%20automatically%20provisioned)). Ensure the switches can boot into ONIE (Open Network Install Environment). If the switches have a different NOS installed, arrange to reboot them into ONIE install mode (for Dell ON series, this is typically available via the bootloader menu). Also connect the data-plane ports according to your desired topology (spine-leaf cabling) as this will be reflected in the wiring diagram.
- **Broadcom ASIC Support:** The custom SONiC image built by Hedgehog includes Broadcom’s SAI (Switch Abstraction Interface) and platform drivers needed for Dell’s Broadcom-based switches. No separate action is needed by the user besides using Hedgehog’s build process – `hhfab` will fetch the appropriate Broadcom support packages or include them in the build (assuming you have the necessary access rights to Hedgehog’s GHCR where those packages reside). Just ensure the switch model is correctly specified so that the build includes the proper Broadcom SDK for that platform.

## Building Custom SONiC OS Images with Hedgehog Fabricator

Hedgehog provides tooling to build a customized SONiC installer image that contains all the software needed for your fabric – including the OS for the control node and the SONiC-based OS for the switches. This image is built as a bootable ISO or IMG that will be used to deploy the control node and automatically provision the switches. We will use the `hhfab` CLI to perform these steps.

**1. Define the Network Topology (Wiring Diagram):** Prepare a **wiring diagram** YAML file describing your fabric topology. This file lists the switches (with identifiers like serial or MAC, their model, and role as spine or leaf), the connections between switches (which ports connect to which), and any external uplinks. You can start from Hedgehog’s example or auto-generate a template. For a quick start, `hhfab` can create a default topology for you in dev mode. For example, to generate a 2-spine, 3-leaf default wiring, run:

```bash
hhfab init --dev
```

This creates a default config file `fab.yaml` and a basic wiring diagram. By default it assumes 2 spines, 2 MCLAG leaf pairs, and 1 single (non-MCLAG) leaf ([Running VLAB - Open Network Fabric](https://docs.githedgehog.com/latest/vlab/running/#:~:text=match%20at%20L109%20By%20default%2C,You)). In a real deployment, instead use `hhfab init --wiring <your_wiring.yaml>` to initialize using your custom wiring file ([Install - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/install/#install-control-node#:~:text=match%20at%20L120%20Hedgehog%20has,The%20first)) ([Install - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/install/#install-control-node#:~:text=HHFAB%20commands%20to%20make%20a,bootable%20image)). After running `hhfab init`, edit the generated `fab.yaml` to verify all details:
   - **Switch entries:** Ensure each switch’s serial/MAC and model (e.g. `Dell S5232F-ON`) is correct in the wiring section. This ensures Hedgehog knows which OS image to apply and can recognize the switch when it boots for provisioning.
   - **Control node settings:** Verify the control node install target disk (e.g. `/dev/sda`) and the NIC name used for management on the control node ([Install - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/install/#install-control-node#:~:text=1.%20%60hhfab%20init%20,and%20control%20node%20NIC%20names)).
   - **Fabric settings:** You can adjust fabric mode (`collapsed-core` vs `spine-leaf`), number of spines/leafs, IP subnets for the underlay, BGP ASNs, etc., or accept defaults. By default, Hedgehog will assign IP subnets and ASN numbers if not explicitly set.

**2. Validate Configuration:** Run: 

```bash
hhfab validate
``` 

This will check the `fab.yaml` and wiring diagram for errors ([Install - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/install/#install-control-node#:~:text=1.%20%60hhfab%20init%20,and%20control%20node%20NIC%20names)) (e.g. missing info or mis-cabled port references). Fix any issues reported before proceeding.

**3. Build the Installer Image:** Now invoke the build process: 

```bash
hhfab build --mode iso
``` 

This downloads all required artifacts (container images, SONiC binary packages for your switch models, etc.) from Hedgehog’s registry and assembles the installer ([Running VLAB - Open Network Fabric](https://docs.githedgehog.com/latest/vlab/running/#:~:text=installer%20using%20,prerequisites%20for%20running%20the%20VLAB)). The `--mode iso` flag produces an ISO image suitable for virtual media or USB (the default is ISO; `--mode img` can be used to create a raw disk image for USB if needed) ([Install - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/install/#install-control-node#:~:text=match%20at%20L138%203.%20,be%20created%20by%20passing%20the)). This step can take some time as it will pull Hedgehog’s Docker images (for the control-plane services) and the base SONiC image for each switch model in your wiring.

Once complete, the output will be in the `./result/` directory. For example, you should see a file named **`control-1-install-usb.iso`** (if your control node is named `control-1`) of several GB in size ([Install - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/install/#fabric-manages-switches#:~:text=The%20installer%20for%20the%20fabric,mount%20it%20via%20virtual%20media)). This single ISO contains everything: the control node OS installer and the SONiC OS binaries for all switch types in the fabric. (The image is large, e.g. ~7.5GB ([Install - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/install/#fabric-manages-switches#:~:text=The%20installer%20for%20the%20fabric,mount%20it%20via%20virtual%20media)), because it packages multiple components and OS images.)

> **Note:** The build process handles Broadcom support automatically. The Dell Broadcom ASIC drivers and SAI libraries for SONiC are bundled into the image. Ensure you had access to download these (via the GHCR login earlier). If building completely offline, you would need those artifacts cached via Zot (see Hedgehog docs on using a pull-through cache for development ([GitHub - githedgehog/fabricator: Hedgehog Open Network Fabric Installer](https://github.com/githedgehog/fabricator#:~:text=Zot%20is%20an%20OCI%20package,as%20part%20of%20development%20process)) ([GitHub - githedgehog/fabricator: Hedgehog Open Network Fabric Installer](https://github.com/githedgehog/fabricator#:~:text=,from%20the%20githedgehog%20github%20repo))).

**4. (Optional) Prepare Virtual Lab Image:** If you are only testing in a virtual lab (containerized environment), you can also build a VLAB installer. In most cases, the same `hhfab build` covers it. Hedgehog’s `hhfab vlab` commands will automatically use the built artifacts to spin up virtual switch instances. (We cover the VLAB deployment later in this guide.)

## Deploying the Hedgehog Fabric Control Node (Fabricator Deployment)

With the installer image built, next you will install the **Fabric Control Node** using that image. This will set up the Kubernetes-based control plane (including the Fabric and Gateway controllers, and supporting services like etcd, DNS, GUI/CLI tools, etc.) on the control node server.

**1. Install the Control Node OS:** You have two main options to boot the installer:
   - **Mount ISO via IPMI:** If your server’s BMC supports mounting virtual media, attach the `control-1-install-usb.iso` to the server and boot from it.
   - **Boot from USB:** Alternatively, write the image to a USB drive. For example, on a Linux machine use `dd` to write the `.img` file: `sudo dd if=control-1-install-usb.img of=/dev/sdX bs=4M status=progress` (replace `/dev/sdX` with your USB device) ([Install - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/install/#fabric-manages-switches#:~:text=1,usb.img%20of%3D%2Fdev%2Fsdc%20bs%3D4k%20status%3Dprogress)). Then insert the USB into the control node and boot from it.

When the control node boots the installer, it will automatically launch the installation process (no manual intervention needed). The Hedgehog installer uses Flatcar Linux’s provisioning service to install the OS to disk. You can monitor progress via the console or IPMI (the installer runs `flatcar-install.service` in the background) ([Install - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/install/#fabric-manages-switches#:~:text=4,and%20begins%20the%20installation%20process)). This process will partition the disk, install the OS, and apply the initial Hedgehog software packages.

**2. Reboot into Hedgehog OS:** Once installation completes, the system will reboot (remove the USB or ISO mount when prompted to avoid re-entering the installer) ([Install - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/install/#fabric-manages-switches#:~:text=5,complete%2C%20the%20system%20automatically%20reboots)). The control node will boot into a Linux OS (based on **Flatcar Container Linux**) and automatically start up the Hedgehog services. There is no traditional login prompt for configuration during normal boot – the system is largely self-configuring. (If you used the `--dev` flag during `hhfab init`, default credentials will be in effect for convenience: e.g. the Linux `core` user password is `HHFab.Admin!` by default in dev mode ([Install - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/install/#install-control-node#:~:text=match%20at%20L197%201,sudo%20user%20with%20password%20%60HHFab.Op)). In production, you should use secure credentials.)

**3. Control Plane Initialization:** The first boot may take a few minutes as Kubernetes services start. The control node will run a Kubernetes cluster (single-node) hosting:
   - **Fabric operator/controller:** Manages fabric CRDs (Connections, VPCs, etc.) and configures the network accordingly.
   - **Gateway operator/controller:** Manages gateway CRDs (External Attachments/Peerings) for external connectivity.
   - **Infrastructure services:** etcd or K3s datastore, possibly monitoring tools. Hedgehog also provides built-in monitoring – e.g., there are Grafana dashboards available for fabric metrics ([Open Network Fabric](https://docs.githedgehog.com/latest/#:~:text=,27)).
   - **DHCP / ZTP services:** The control node will also act as a DHCP/TFTP server on the management network to facilitate zero-touch provisioning of switches.

Once up, the control node will automatically start listening for the switches coming online. At this point, ensure the control node’s **management interface has network connectivity** to the switch management ports. If the installer didn’t pre-configure the management NIC’s IP (it may have if `fab.yaml` had details), you might need to assign an IP manually. Typically, the Hedgehog installer configures the management NIC as **DHCP client** by default, expecting the user to have either a DHCP server on the mgmt network or to reconfigure as needed. You can use the serial console to set a static IP if required (using Flatcar’s network configuration).

**4. Verify Control Node Services:** It’s useful to verify that the control plane is running properly:
   - Try accessing the Hedgehog Fabric CLI. The `hhctl` or `kubectl` command on the control node can be used to list custom resources. For example: `kubectl get switches` or `hhctl fabric status`. (Refer to Hedgehog documentation for CLI usage ([External Peering - Open Network Fabric](https://docs.githedgehog.com/latest/user-guide/external/#:~:text=,Using%20VPCs%20with%20Harvester)).)
   - If a GUI or dashboard is provided (check Hedgehog docs), you might also use that to verify the fabric status.

At this stage, the fabric control node is ready and waiting to bring the switches into the fabric.

## Provisioning the Dell Switch Dataplane (Deploying SONiC to Switches)

With the control node up, the next step is to **deploy the custom SONiC OS onto each Dell switch** and have them join the fabric. Hedgehog’s system is designed for **zero-touch provisioning**: the control node will automatically install the OS on each switch via ONIE and configure it. Ensure all switches are connected to the management network and powered on.

**1. Boot Switches in ONIE Install Mode:** If the Dell switches are new or have no OS, they will boot into ONIE by default. If an OS is present, interrupt the boot and select ONIE install. The goal is to have each switch PXE-boot or DHCP-boot via ONIE so it can fetch the installer. The control node is prepared to serve the SONiC installer image for each switch model. As each switch boots ONIE, it will DHCP on the mgmt interface:
   - The Hedgehog control node’s DHCP service will recognize the switch (matching the MAC or serial to the wiring diagram ([Install - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/install/#fabric-manages-switches#:~:text=4,to%20have%20them%20automatically%20provisioned))) and provide it the location of the correct installer image.
   - ONIE will then download the image (usually via HTTP/HTTPS from the control node) and proceed to install the SONiC-based OS on the switch automatically ([Install - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/install/#fabric-manages-switches#:~:text=2,to%20have%20them%20automatically%20provisioned)).

This process may take several minutes per switch. You can monitor progress from the switch console (it will show ONIE retrieving an image and installing Linux/SONiC).

**2. Switch Hedgehog OS Boot:** After ONIE installs the Hedgehog-provided SONiC image, the switch will reboot into SONiC. This is a special SONiC build customized for Hedgehog:
   - It includes the standard SONiC infrastructure (Linux base, SAI for Broadcom, SONiC management framework).
   - It adds the **Hedgehog Dataplane Agent** service. This agent (written in Rust) runs on the switch either as a container or native service, and connects back to the control node. Its job is to apply configurations received from the Fabric controller (via gRPC or similar RPC, defined in `dplane-rpc` ([Hedgehog · GitHub](https://github.com/githedgehog#:~:text=%2A%20dplane))) to the switch. For example, when you create a VPC and attachments via the control plane, the agent on each relevant switch will configure VLANs, VRFs, BGP sessions (likely through FRR or direct ASIC programming), VXLAN tunnel endpoints, etc., on that switch.
   - Optionally, the switch may also run a **Kubernetes kubelet** in future to fully integrate into the cluster (as noted in Hedgehog docs) ([System Requirements - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/requirements/#:~:text=,Agent%20that%20controls%20devices%20configuration)), but currently the agent can operate independently to receive instructions.

Each switch will come up with two local user accounts created by Hedgehog (if in dev mode, the defaults are `admin` with password `HHFab.Admin!` and `op` with `HHFab.Op!` ([Install - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/install/#install-control-node#:~:text=match%20at%20L197%201,sudo%20user%20with%20password%20%60HHFab.Op))). These credentials allow direct login for troubleshooting, but all configuration should be orchestrated via the Hedgehog fabric management – you typically do not configure the switches by hand.

**3. Switches Join the Fabric:** When the switches boot, they will register with the control node:
   - The Hedgehog fabric controller will detect each switch (matching by serial or MAC from the wiring diagram) and mark it as part of the fabric. You can run `kubectl get switches` (or `hhctl`) on the control node to see the status of each switch (they may appear as CRDs with a status field indicating success or any errors).
   - The controller will then push initial configuration to set up the **underlay network**. This usually means configuring BGP sessions between leaves and spines (for EVPN underlay) and enabling any required base protocols (LLDP, PTP, etc.). The Dell switches, running SONiC, will have FRRouting (FRR) running for BGP. Hedgehog likely automates FRR config via the agent using an OpenConfig model or SONiC’s management framework, so you might see BGP neighbors form automatically (spines and leaves exchanging EVPN routes).

At this point, your physical fabric should be up and running under Hedgehog’s control:
   - The **underlay** (IP fabric with BGP) is established between spines and leaves ([Fabric Implementation - Open Network Fabric](https://docs.githedgehog.com/latest/architecture/fabric/#:~:text=Underlay%20Network)).
   - EVPN is configured so that the stage is set for overlay networks (VPCs). The fabric uses EVPN to advertise MAC/IP endpoints and VXLAN VNI mappings between switches ([Fabric Implementation - Open Network Fabric](https://docs.githedgehog.com/latest/architecture/fabric/#:~:text=standard%20BGP%20EVPN%20and%20VXLAN,Fabric%20provides%20isolation%20between%20different)).
   - The Hedgehog Fabric agent on each switch will also manage switch-specific settings like enabling PFC (Priority Flow Control) or ECN if needed for AI/ROCÉ congestion management (part of Hedgehog’s performance tuning features), as well as setting up any MLAG (MCLAG) pair relationships if you have multi-chassis LAGs between leaf pairs.

**4. Configure Fabric Overlay (VPCs and Attachments):** At this stage, you can begin creating **VPCs, subnets, and attachments** through Hedgehog’s Kubernetes API:
   - Define `VPC` custom resources to represent isolated layer-3 networks for tenants or environments.
   - Define `Namespace` (VRF contexts) if using multiple VRFs.
   - Define `Attachment` or `Connection` resources to attach servers or devices to the fabric (for example, a Connection might represent a server NIC connected to a leaf port, associated with a particular VPC). In Hedgehog, servers and switches are represented in the API (e.g., a `Server` and a `Connection` CRD link a server port to a switch port and VLAN).
   - When these are applied, the controllers will instruct the switch agents to create the necessary VLANs or VXLAN mappings. For instance, if you attach a server port to VPC A on a given switch port, the agent may create a VLAN subinterface on that switch port and map it to a VXLAN VNI for VPC A, and ensure EVPN advertises that segment.

Hedgehog’s fabric thus automatically translates high-level network intent into the appropriate SONiC configurations on each Dell switch. You can refer to the Hedgehog **Fabric CLI** or Kubernetes CRDs for detailed configuration syntax ([External Peering - Open Network Fabric](https://docs.githedgehog.com/latest/user-guide/external/#:~:text=,Using%20VPCs%20with%20Harvester)). Also see the **Switch Profiles Catalog** in the Hedgehog docs for port naming conventions per model ([Supported Devices - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/supported-devices/#:~:text=You%20can%20find%20detailed%20information,the%20User%20Guide%20%2025)) – e.g., on S5232F-ON, port names might be Ethernet1/1, Ethernet1/2, etc., which should match your wiring diagram port names.

## Enabling Gateway Services (External Connectivity)

If your fabric needs to connect to external networks or the Internet, you will use Hedgehog’s **Gateway** components. There are a few ways to deploy gateway functionality:

**1. Using a Border Leaf as a Gateway:** In many cases, one (or more) of your fabric switches at the edge (typically a spine or dedicated border leaf) will connect to an external router or upstream network. Hedgehog can manage this by defining an **External Attachment** on that switch port and an **External Peering** policy. In the wiring diagram or fabric configuration, you’d mark that a certain switch port (say, port 1/52 on a spine) connects to an “Edge” device ([External Peering - Open Network Fabric](https://docs.githedgehog.com/latest/user-guide/external/#:~:text=match%20at%20L420%20,allowing%20inbound%20routes%20from%20it)). For example, you might add an `ExternalAttachment` entry linking `Spine1:Ethernet52` to an external peer (named “EdgeRouter1”). 

   - The **Gateway controller** running on the control node will detect this and create the necessary configuration on Spine1: it will allocate an IP address for the spine’s external interface if needed and configure a BGP session (or static route) to the EdgeRouter.
   - Hedgehog’s gateway agent (likely running on the same switch as part of the fabric agent or FRR) will program the BGP peering. This typically uses eBGP to exchange routes between the fabric and the external router.
   - On the external device side, you need to configure a matching BGP session. Hedgehog documentation provides an example external BGP config (for instance, using SONiC on the external side as well) ([External Peering - Open Network Fabric](https://docs.githedgehog.com/latest/user-guide/external/#:~:text=,32)). Essentially, the external router will see the fabric’s advertised routes (e.g. VPC subnet routes) and the fabric will learn external routes (e.g. default route) through this peering.

   Using this method, the fabric’s VPCs can be selectively exposed externally. Hedgehog’s `ExternalPeering` CRD can be used to specify which VPC subnets are exported to the external peer and which external routes are imported ([External Peering - Open Network Fabric](https://docs.githedgehog.com/latest/user-guide/external/#:~:text=,allowing%20inbound%20routes%20from%20it)). This gives fine-grained control – for example, you can expose only a “public” VPC to an external transit gateway while keeping others fully isolated.

**2. Dedicated Gateway Appliance:** In some scenarios, you might use a dedicated software gateway (for example, a VM running a virtual router for NAT, firewall, or load balancing). Hedgehog is designed to also manage such gateways. The Gateway agent could run on that dedicated node to program it via APIs. However, for the scope of Dell Broadcom switches, typically the gateway will be one of those switches as described above, or another SONiC device. (If you were to deploy a standalone x86 gateway, you would treat it as a special “Gateway Node” managed by Hedgehog – but current open documentation focuses on external peering via switches.)

**3. Gateway Features:** Hedgehog’s gateway services are not limited to simple BGP peering. The platform is intended to provide cloud-like gateway functions such as transit routing between VPCs and outside, load-balancer as a service, and network security policies ([Hedgehog AI Network](https://hedgehog.cloud#:~:text=Hedgehog%20offers%20a%20cloud%20user,transit%2C%20load%20balancing%20and%20security)). As of now, the primary implemented feature is likely BGP transit (route exchange). Future or enterprise versions may add containerized network functions (CNFs) for firewalls or load balancers integrated with the fabric.

To configure external connectivity in Hedgehog:
   - Define an **External** object (identifying the external domain or router).
   - Define an **External Connection** linking a fabric `Connection` (switch port) to that External.
   - Define an **ExternalPeering** resource to allow specific VPC routes to be advertised externally ([External Peering - Open Network Fabric](https://docs.githedgehog.com/latest/user-guide/external/#:~:text=,allowing%20inbound%20routes%20from%20it)).
   - The Gateway controller will ensure the switch’s BGP (in FRR) is configured with a neighbor for the external peer and apply route policies per the CRD specification.

After applying these, monitor on the control node (via `kubectl get externalpeerings` or similar) and on the switch (e.g., check FRR BGP summary) to ensure the external BGP session is established. Once up, your fabric’s networks should be reachable to the outside as configured. For example, if you created a VPC with subnet 10.10.0.0/16 and allowed it in ExternalPeering, the external router will learn 10.10.0.0/16 via BGP, and the fabric will likely learn a default route (0.0.0.0/0) or other prefix from the external.

**Note:** The Dell switches (running SONiC) are fully capable BGP routers, and Hedgehog leverages that. Ensure your external router’s settings (BGP ASN, authentication if any, etc.) match what you configure in Hedgehog. Also, consider redundancy – you might connect two border leaves to upstream for HA, and Hedgehog can handle multiple ExternalAttachments (each switch will peer with the upstream and the external side can use BGP multipath).

## Deployment in a Virtual Lab (Container-Based Workflow)

Hedgehog supports deploying the entire fabric in a **Virtual Lab (VLAB)** environment, which is ideal for testing and learning without physical hardware. In this mode, both the control plane and simulated switches run on a single host (or a few hosts) using virtualization (QEMU/KVM or containers). This is a container-based workflow in the sense that the switch NOS instances and networking are virtualized.

**Key differences in VLAB:**
- Virtual switches (data plane) are simulated, often using network namespaces or lightweight VMs running SONiC or an equivalent switch model. The Fabricator will create virtual machines to represent spines and leaves.
- The control node can run as a VM or as just processes on your host.
- All networking is emulated (usually with TAP interfaces and software bridges to connect VMs).

**Steps to use the Virtual Lab:**

1. **Initialize and Customize Topology:** On your Linux machine (with KVM enabled), run `hhfab init --dev` to create a default dev config ([Running VLAB - Open Network Fabric](https://docs.githedgehog.com/latest/vlab/running/#:~:text=First%2C%20initialize%20Fabricator%20by%20running,help)). Edit `fab.yaml` if you want to change the number of virtual spines/leafs or topology mode (by default, `hhfab init --dev` sets up a 2-spine, 3-leaf topology with MCLAG on two leaf pairs ([Running VLAB - Open Network Fabric](https://docs.githedgehog.com/latest/vlab/running/#:~:text=By%20default%2C%20,You))). You can also adjust the `mode` to `collapsed-core` in the fab.yaml if you prefer a single-tier topology ([Running VLAB - Open Network Fabric](https://docs.githedgehog.com/latest/vlab/running/#:~:text=match%20at%20L134%20If%20a,core)).

2. **Generate the Virtual Wiring Diagram:** Run `hhfab vlab gen` to generate the virtual wiring and VM definitions ([Running VLAB - Open Network Fabric](https://docs.githedgehog.com/latest/vlab/running/#:~:text=By%20default%2C%20,You)). You can customize parameters (like `--mclag-leafs-count` or `--spines-count`) to simulate different topologies ([Running VLAB - Open Network Fabric](https://docs.githedgehog.com/latest/vlab/running/#:~:text=match%20at%20L152%20ubuntu%40docs%3A,spinesCount%3D2%20fabricLinksCount%3D2)).

3. **Build the VLAB installer:** Run `hhfab build` (as above) to prepare the images. This will pull required artifacts and create an installer image for the virtual switches and control node, just like for hardware ([Running VLAB - Open Network Fabric](https://docs.githedgehog.com/latest/vlab/running/#:~:text=installer%20using%20,prerequisites%20for%20running%20the%20VLAB)). (Often the VLAB uses a slightly different SONiC image optimized for virtual use, but `hhfab` handles this.)

4. **Launch the Virtual Lab:** Execute `hhfab vlab up`. This command will spin up the virtual control node and switch VMs as defined ([Running VLAB - Open Network Fabric](https://docs.githedgehog.com/latest/vlab/running/#:~:text=To%20build%20and%20start%20the,Ctrl%20%2B%20C)). The `vlab up` process will:
   - Start a virtual control node VM and install it (similar to the ISO process).
   - Start each virtual spine/leaf VM and provision them (the console output of `hhfab vlab up` will show each VM being created and configured).
   - Connect the VMs’ interfaces to simulate the cabling (spine-to-leaf links, etc., are emulated with bridge networks).

   By default, `hhfab vlab up` runs in the foreground, showing logs. It’s helpful to watch these logs to ensure all VMs boot correctly ([Running VLAB - Open Network Fabric](https://docs.githedgehog.com/latest/vlab/running/#:~:text=To%20build%20and%20start%20the,Ctrl%20%2B%20C)). You can add the `--kill-stale` flag to ensure any old lab VMs are cleaned up first.

5. **Accessing the Virtual Fabric:** Once all VMs are up, your virtual fabric is running. You can access the control node VM (often via SSH or virsh console – the default user might be `core` as with Flatcar). From there, use the same Hedgehog CLI commands to manage the fabric (the VLAB environment is functionally identical to hardware). The documentation provides default credentials for the VLAB environment (for instance, in dev mode, `core/HHFab.Admin!` on the control node, and `admin/HHFab.Admin!` on switches) ([Install - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/install/#install-control-node#:~:text=match%20at%20L197%201,sudo%20user%20with%20password%20%60HHFab.Op)) ([Install - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/install/#install-control-node#:~:text=1.%20If%20the%20insecure%20%60,sudo%20user%20with%20password%20%60HHFab.Op)).

6. **Testing in VLAB:** You can now create VPCs, attach virtual servers (the VLAB may simulate servers as additional VMs or simply allow you to ping between switches to test connectivity). For a quick demo, Hedgehog provides sample configs (see *Demo on VLAB* in docs). Use `kubectl` on the control node to create a few VPCs and observe that the virtual switches establish connectivity (you can enter the virtual SONiC CLI in a switch VM or use `show bgp summary` to see EVPN sessions, etc.).

7. **Tearing down and repeating:** When done, you can stop the lab with Ctrl+C (which stops `vlab up`) or by running `hhfab vlab down` if such command exists. To reset the lab completely, use `hhfab vlab reset` to destroy and clean up VMs ([Running VLAB - Open Network Fabric](https://docs.githedgehog.com/latest/vlab/running/#:~:text=,Next%20steps)). This allows you to iterate quickly – edit config, rebuild (if needed), and bring the lab up again.

**Use Cases for Container-Based Deployment:** The virtual lab is excellent for experimenting with Hedgehog features, simulating different topologies, and training purposes. It’s also a good CI environment – you could integrate `hhfab vlab` in a pipeline to test network changes via GitOps (Hedgehog has a demo GitOps repository illustrating managing network configs declaratively). Keep in mind that performance in VLAB is limited (software switching), so it’s not suitable for performance testing, but functionally it mirrors the real deployment.

## Configuration Example: Fabric Topology and CRDs

To solidify understanding, here’s a simplified example of a **fabric configuration** for a small deployment using Dell switches:

- **Topology:** 1× Dell S5232F-ON as spine, 2× Dell S5248F-ON as leafs (forming a MLAG pair for dual-homed servers).
- **Wiring Diagram (excerpt):**

| Device        | Role   | Model         | ID (Serial/MAC)    | Connections                   |
|---------------|--------|---------------|--------------------|-------------------------------|
| Spine1        | spine  | Dell S5232F-ON | <SERIAL_SPINE1>    | Eth1/1 → Leaf1 Eth1/49, Eth1/2 → Leaf2 Eth1/49 |
| Leaf1         | leaf   | Dell S5248F-ON | <SERIAL_LEAF1>     | Eth1/49 → Spine1 Eth1/1, Eth1/50 → Leaf2 Eth1/50 (ISC) |
| Leaf2         | leaf   | Dell S5248F-ON | <SERIAL_LEAF2>     | Eth1/49 → Spine1 Eth1/2, Eth1/50 → Leaf1 Eth1/50 (ISC) |
| **ExtRouter** | external | n/a           | (peer IP)         | Spine1 Eth1/32 → ExtRouter (Ethernet link) |

In this example, two leafs (Leaf1 & Leaf2) form an MLAG pair (connected by an Inter-Switch Link on Eth1/50) to provide redundant server connections. The spine connects to both leaves. Additionally, Spine1 has an external uplink on port Eth1/32 to an external router.

- **Fabric `fab.yaml`:** Would specify `fabricMode: spine-leaf`, `spinesCount: 1`, `mclagLeafsCount: 1` (since we have one MLAG pair), and list the device profiles for the S5232F and S5248F with their identifiers. It would also define an ExternalAttachment for Spine1 Eth1/32.

- **After deployment**, you would create CRDs such as:
  - `Server` for each server connected (with a reference to which leaf ports it’s connected to).
  - `Connection` for each server-to-leaf link (associating a server interface with a leaf port).
  - `VPC` for a tenant network, and `VPCAttachment` binding that VPC to the `Connection` (so the server port is in that VPC).
  - `External` for the external router, `ExternalAttachment` for Spine1’s port to that router, and `ExternalPeering` to allow certain VPCs to route externally.

For instance, an `ExternalPeering` CR could specify that VPC “ProdNet” is exported to external “ExtRouter” with a certain route policy ([External Peering - Open Network Fabric](https://docs.githedgehog.com/latest/user-guide/external/#:~:text=,allowing%20inbound%20routes%20from%20it)). Hedgehog’s controllers would then configure Spine1’s BGP to announce “ProdNet” subnet routes to ExtRouter and accept incoming routes.

The above configuration can be managed via YAML manifests and applied with kubectl, or using Hedgehog’s CLI which simplifies creating these resources.

*(Refer to Hedgehog’s official documentation for detailed YAML examples of each CRD and the Switch Profile Catalog for exact port naming on Dell models ([Supported Devices - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/supported-devices/#:~:text=You%20can%20find%20detailed%20information,the%20User%20Guide%20%2025)).)*

## Best Practices for Production and Learning Environments

Finally, keep in mind the following best practices to ensure a smooth deployment and operation:

- **Use a Dedicated OOB Network:** Always use a separate Out-of-Band management switch/network for the control node and device management ports ([System Requirements - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/requirements/#:~:text=In%20order%20to%20provision%20and,connect%20to%20the%20control%20node)). This network should be isolated from your data plane to prevent any interference. In production, secure this network (firewall it from corporate LAN, use management VLANs, etc.) since it has control over all switches.
- **Plan IP/Subnet Allocation:** Hedgehog can auto-assign subnets for loopbacks and inter-switch links, but for clarity and compliance with your IP plan, you may want to specify them in `fab.yaml`. Ensure the underlay IP subnetting (for p2p links and loopbacks) has enough space for all links. Also decide on BGP ASN assignments (you can use private ASNs for your fabric).
- **Leverage Switch Profiles:** When editing wiring diagrams or fabric config, use the correct **port naming** for each switch model as per the Hedgehog Switch Profiles. For example, Dell S5248F-ON might use interface names like `Ethernet1/1` etc. The Hedgehog docs’ Switch Profiles Catalog ([Supported Devices - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/supported-devices/#:~:text=You%20can%20find%20detailed%20information,the%20User%20Guide%20%2025)) provides these. Consistent naming prevents deployment errors.
- **Avoid Dev Mode in Production:** The `--dev` flag in `hhfab init` is convenient for lab setups (it uses default passwords and some insecure defaults). For a production fabric, omit `--dev` and supply your own secure passwords/API tokens in the config. Change any default credentials immediately after initial setup if they were used.
- **Resource Monitoring:** Ensure your control node has adequate resources (CPU, memory). It not only runs the controllers but also an etcd/DB and possibly monitoring stack. 32GB RAM is a safe recommendation ([System Requirements - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/requirements/#:~:text=,Micron%207450%20MAX%20400GB%20NVMe)). Monitor the control node’s resource usage, especially if managing a large number of switches. If needed, scale up the hardware or plan for an HA control plane (when supported).
- **Firmware and Diagnostics:** Before deploying Hedgehog SONiC on Dell switches, it’s wise to upgrade each switch’s ONIE and BIOS/Firmware to the latest from Dell. This can prevent known issues and ensures compatibility with SONiC. Also make note of how to access ONIE console for each model in case you need to recover a switch or re-install.
- **Post-Install Verification:** After the fabric comes up, verify routing adjacencies and VLAN/VXLAN configs:
   - Check BGP EVPN sessions on spines/leafs (e.g. `show bgp evpn summary` in SONiC CLI).
   - Use Hedgehog’s fabric CLI (`hhctl`) to list VPCs, Connections, etc., and ensure the state is `Active`.
   - If using MLAG (MCLAG), verify that peer link is up and the two leafs are in sync (SONiC will use ICCP or similar – check `show mclag brief` if available).
   - If using external peering, verify routes are propagating out and in as expected (check route tables in SONiC via `show ip route vrf <vrfName>`).
- **High Availability Considerations:** Hedgehog’s current open version uses a single control node (non-HA) ([System Requirements - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/requirements/#:~:text=,Upgrade)). In critical environments, ensure the control node is on a UPS and ideally backed up. Hedgehog’s dataplane will continue forwarding even if the control node is temporarily down, but you won’t be able to make changes. Plan maintenance windows for the control plane accordingly. The documentation hints at future HA support (multiple control nodes) ([System Requirements - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/requirements/#:~:text=,Upgrade)) – keep an eye on updates if you require redundancy.
- **Use Grafana and Logs:** Hedgehog provides Grafana dashboards for fabric health ([Open Network Fabric](https://docs.githedgehog.com/latest/#:~:text=,29)). Set these up to monitor switch port statuses, BGP session states, latency, etc. Also, familiarize yourself with where Hedgehog logs events (likely in Kubernetes logs for the fabric and gateway controllers, and on switches possibly via syslogs). This will help in troubleshooting.
- **Practice in the Lab:** For learning environments, take full advantage of the Virtual Lab. Experiment with adding/removing VPCs, simulating link failures (you can shut interfaces in the virtual switches via SONiC CLI to see how fabric reacts), and even upgrading the fabric software through `hhfab upgrade` procedures ([Install - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/install/#:~:text=,15)) ([Install - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/install/#:~:text=,Upgrade)). This will build confidence before touching production.
- **Automation and GitOps:** Treat your network config as code. Hedgehog’s use of Kubernetes CRDs means you can store the desired network state in YAML files. Use a Git repository to version control your `fab.yaml`, wiring diagrams, and CRDs (VPC, Connection definitions). You can even use GitOps tools (Argo CD, Flux) to automatically apply changes to the Hedgehog control plane. This practice reduces human error and makes rollbacks easier. (Hedgehog’s `demo-gitops` repo is an example showing how network intent can be managed via Git commits.)
- **Stay Updated:** Hedgehog is an active open-source project. Check the release notes and upgrade guide ([Install - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/install/#:~:text=,15)) ([Install - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/install/#:~:text=,Upgrade)) for new features or fixes, especially concerning Broadcom support and Dell platforms. When upgrading the fabric or switch images, follow the official procedure (`hhfab upgrade` can assist with a smooth upgrade of control node and orchestrated upgrade of switches with minimal downtime ([Install - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/install/#:~:text=,17))).

By following this guide, you should be able to build the Hedgehog SONiC images for Dell Broadcom-based switches and deploy a robust leaf-spine fabric with cloud-like network virtualization. Hedgehog provides the flexibility of a modern, intent-driven network (through Kubernetes APIs) while leveraging the performance of Dell’s Broadcom switch hardware without vendor lock-in ([Hedgehog · GitHub](https://github.com/githedgehog#:~:text=The%20Hedgehog%20Open%20Network%20Fabric,in)). Whether in a production data center or a learning lab, Hedgehog’s combination of **custom SONiC, Kubernetes automation, and standard protocols** (EVPN VXLAN) offers a powerful platform for distributed cloud networking.

**Sources:** The steps and best practices above are based on the official Hedgehog documentation and GitHub repositories ([Download - Open Network Fabric](https://docs.githedgehog.com/latest/getting-started/download/#:~:text=The%20main%20entry%20point%20for,or%20run%20the%20Virtual%20LAB)) ([Install - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/install/#fabric-manages-switches#:~:text=1.%20%60hhfab%20init%20,be%20created%20by%20passing%20the)) ([Supported Devices - Open Network Fabric](https://docs.githedgehog.com/latest/install-upgrade/supported-devices/#:~:text=,C4632SB)) ([Hedgehog AI Network](https://hedgehog.cloud#:~:text=Hedgehog%20offers%20a%20cloud%20user,transit%2C%20load%20balancing%20and%20security)), which provide detailed reference on supported devices, installation procedures, and architectural concepts of the Hedgehog Open Network Fabric.